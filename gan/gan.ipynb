{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwUncnSNLtnw"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "epochs = 100\n",
        "z_dim = 100\n",
        "image_size = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "path = kagglehub.dataset_download(\"splcher/animefacedataset\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "class AnimeFaceDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.image_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_files[idx])\n",
        "        return self.transform(image) if self.transform else image\n",
        "\n",
        "dataset = AnimeFaceDataset(os.path.join(path, 'images'), transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, 512, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z.view(z.size(0), z.size(1), 1, 1))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).view(-1, 1)\n",
        "\n",
        "G = Generator(z_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, real_imgs in enumerate(dataloader):\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        z = torch.randn(batch_size, z_dim).to(device)\n",
        "        fake_imgs = G(z)\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = criterion(D(real_imgs), real_labels)\n",
        "        fake_loss = criterion(D(fake_imgs.detach()), fake_labels)\n",
        "        d_loss = real_loss + fake_loss\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss = criterion(D(fake_imgs), real_labels)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{epochs}] Batch {i}/{len(dataloader)} Loss D: {d_loss:.4f}, Loss G: {g_loss:.4f}\")\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(64, z_dim).to(device)\n",
        "            fake_imgs = G(z).cpu()\n",
        "            grid = make_grid(fake_imgs, nrow=8, normalize=True)\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.imshow(grid.permute(1, 2, 0))\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(G.state_dict(), f\"generator_epoch_{epoch}.pth\")\n",
        "torch.save(D.state_dict(), f\"discriminator_epoch_{epoch}.pth\")"
      ],
      "metadata": {
        "id": "PHqwpoMaRGhk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}